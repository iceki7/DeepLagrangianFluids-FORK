torch环境有无
    一键执行失败
    


数据生成
    dpi-net 
    splish


【数据格式
    zst 
    这个格式是不是作者写过一个接口，既然它能用很多种生成方式得到的话？
        有从numpy转换为zst的接口
    总之要么从zst之前就送入数据。要么绕过zst送入数据。
    绕过更方便。

    湍数据因为手动加载，yaml在训练脚本中不起作用了。
    yaml只是指出数据位置。



待学习的数据里 涡度的效果就必须要明显才行 最好是弄些具有代表性的场景
然后学习的时候要把多个场景作为训练数据
（因为一个batch里其实包含了很多个不同的场景，应该是为了方法的泛化）
而每个场景里其实只取了3帧数据。



涡度数据是3维的。学习成本。

【dataset】


    随机缩放
   
    场景一多，那每个场景取多少帧？
        人工选择一些比较关键的帧，但是太麻烦了，因为是多场景。
        固定等间距采样。

    怎么把每个场景的参数传进去？每个场景有哪些参数？
        场景类型。（下落，冲击半球）
        流体数据和边界数据直接传。
        边界法向量自己编写一个，做一个近似。



    需要随机生成一堆的json。然后用生成出的这些全部跑一遍。
    既然如此法向量还是在模拟框架中生成比较好，就不用把模拟参数传到训练框架里了
    先随意跑场景，然后肉眼筛掉那些因为碰到边界而炸掉的场景就行了

    每个场景的粒子数量大致是多少
        看之前读入数据之后的print

    因为每次梯度回传的时候，损失函数只是关于【3帧之间的】
    所以哪怕每个场景只取15帧，问题也不大


    更复杂的场景的法向量?
        球体还可以数学建模一下
    峡谷场景是怎么来的 因为最后泛化肯定是希望能泛化到复杂场景上
        obj


    200个场景。哪怕每个场景里所取的帧数少一点都没关系，
    因为在训练的时候只有相邻的三帧会参与损失函数的计算 
    所以其实这些数据是否来自一个场景 关系都不大
    增加场景数还是为了提高泛化性


【定量指标】

frame-pair error
loss


【shell脚本使用方法】
    数据场景+物理模拟结果。
        our_default_scene。逐帧结果在/partio
        sim_0002只是表示一个随机种子。它最后会用在train/valid划分中。
        sim0001 和sim0002 水块形状都不一样 它是不同的场景编号

    压缩数据的结果。
        our_default_data里。
        sim_xxxx_split 



【任务】

卷积的是自定义特征 包括速度
根据距离远近加权。
先将球形领域映射到方块


虽然CCONV提出时是用来做预测的
但从网络原理上来说，做重建应该也是可以的
只是对于粒子位移进行一个修正，但这个修正的含义不一定非得是下一帧的位置 也可以是对涡度的增强


如果没有泛化性，它对于一个不同的场景就又得训练1次。
这样就失去了网络的意义了，也即训练一次，对于多个场景都能进行预测。



【训练】
./train_network_tf.py lowfluid.yaml
    1.按原网络的方法，是在多个场景上训练少量帧。每个场景具有不同尺寸的水块和box。
        our_default_data：200×15zst,每个zst三帧？
    
    2.在单个场景上随机截取多帧（每一帧都由pos0,pos1,pos2组成）。训练的时候会迭代1次计算loss。
        写一个程序随机截取若干个段（每个段有3帧）。
    就选湍流比较明显的那些帧数里，等间距采样段

    vel0指的是帧在solver中的速度，
    而非帧与帧之间的平均速度（因为帧与帧之间已经历经了多个step）
    
    box用法向量还是粒子
        1.如果box不是粒子而是平面，就要改网络结构。能不能用都是问题，毕竟改了结构。此外法向量的信息肯定没有粒子坐标的信息多。
        而且这样不规则的边界处理不了

        2.造一个和边界差不多大的box粒子出来。粒子距离的问题先不管了

        论文中说的是normal。
        但是输入卷积的最底层包括粒子和法向量。是否信息有些冗余？
        但还是先不改。保留网络原貌吧。

        MCVSPH好像没有提供固体粒子边界处理，是用法向量处理的。
        用之前的框架搞box。
            生成box的方法有问题。重写。

        可不可以用splishSplash搞？cconv自带的
            需要自行提供box.obj。应该是建模得来的？转换成粒子和norm确实是可以是有封装。
            其实用taichi-SPH弄个box以及法向量也没那么难，因为代码逻辑是透明的。主要是繁琐。但是上述这个有封装的代码，还需要调试。

        其实就是一个理论上的数组就可以了。不用真的去用框架造出来。用numpy造都行。


有ckpt就会restore。（文件夹里）




batchSize越大，网络回传一次就要考虑越多的信息。按理来说收敛速度就会越慢。
但是同一套数据不应该重复出现在一个batch里吧？一般来说都是数据量比较大的时候分多个batch训练才对。

可能还是因为数据规模的问题。
或者完全照搬原来的网络的训练集大小以及超参数。



【训练问题】

    暂停恢复会让loss发生抖动
    是因为每次读入数据时遍历的第一次从文件夹里读，之后都是从数组里读所导致的吗

    学习过程是否要包括滞空的那段时间？
    网络施加重力，按理来说已经是准确的了，不需要增加修正。
    所以需要通过数据告诉网络：如果领域里没有边界，也没有表面，那网络的修正量就是0。


【可选策略】

    随机旋转。

    重新设计了粒子权重。对于这种飞溅出去的粒子重要性降低
    但是仍然保留了原先权重考虑表面特征的属性。
    这也和核半径大小到底是多少有关。
    或者考虑在数据集里就去掉这部分粒子



    或者粒子领域粒子过少时就不修正了



    更稀疏的场景，精细化学习。


【训练时间】
    从作者的数据来看不用到5w，只要数据对了，很前面就能看出结果了

    训练速度和每个seg里的粒子数有关系。

    多开几张卡

【数据集】

default_data上来默认的loss就只有2。
它训练2800时已经有水块下落的基本效果了
200个场景 15帧


lowfluidS39     seg 1240
lowfluidS100    seg 3320    8h      平均粒子数量4k



数据集是4个dt一帧。
相邻帧之间隔了多个step。但速度数据并不是帧间的平均速度。
    1.要么就每个step输出一帧
    2.要么就计算现在两帧之间的平均速度（数据量更少。）


参数对齐：
    场景尺寸
    dt
    radius √
    g √


每个batch 16组，每组3帧。



【训练结果】

S39 S100
    不对称性

    似乎消除了作者的训练结果中水块下落时的不真实性

S100
    泛化到峡谷,大趋势有
    C:\Users\123\Downloads\canyon_S100\fluid_*.npz
    可能是训练集里飞溅就过于剧烈所导致的


    峡谷 pre
    D:\CODE\CCONV\DeepLagrangianFluids\scenes\canyon_outx\fluid_*.npz



    

    每个场景取多少数据？保持和原网络的数据规模一致
    
    每个场景中随机性的波动范围有多大
    模拟过程用splish,splash完成，传进去的接口参数有哪些

    主要还是水块下落，随机放缩。
    也增加与半球交互的场景。
    体现湍流特征的数据。

    可能还是要更仔细地先观察MCVSPH之类的方法产生的数据有什么效果。
    这样才能预期之后要学习到的效果是什么。并且制定相应的数据集。
    因为还是针对湍流的学习，说到底。




【评估模型的损失】
/evaluate_network.py --trainscript train_network_tf.py --cfg default.yaml




训练数据：
    粒子位置，速度，（涡度）
    预测:位置矫正，速度矫正，涡度矫正。
损失函数：
    涡量损失。对于求解出的速度场求涡度。





